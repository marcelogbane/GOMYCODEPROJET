{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43a331-9f3c-4fdd-ac59-68f20f4406fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4\n",
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671ca556-28f4-4693-a189-798264245fbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# 1.6) Test de la fonction sur une page Wikipédia de votre choix\u001b[39;00m\n\u001b[0;32m     69\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/wiki/Web_scraping\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Exemple de page Wikipédia\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m scrape_wikipedia(url)\n",
      "Cell \u001b[1;32mIn[4], line 43\u001b[0m, in \u001b[0;36mscrape_wikipedia\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_wikipedia\u001b[39m(url):\n\u001b[1;32m---> 43\u001b[0m     soup \u001b[38;5;241m=\u001b[39m get_wikipedia_html(url)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m soup:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;66;03m# Extraction du titre\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         title \u001b[38;5;241m=\u001b[39m get_article_title(soup)\n",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m, in \u001b[0;36mget_wikipedia_html\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_wikipedia_html\u001b[39m(url):\n\u001b[1;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m      5\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "# 1.1) Fonction pour obtenir et analyser le contenu HTML d’une page Wikipédia\n",
    "def get_wikipedia_html(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(f\"Impossible de récupérer la page. Code d'erreur : {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# 1.2) Fonction pour extraire le titre de l’article\n",
    "def get_article_title(soup):\n",
    "    title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "    return title\n",
    "\n",
    "# 1.3) Fonction pour extraire le texte de chaque paragraphe avec leurs rubriques\n",
    "def get_article_content(soup):\n",
    "    content = {}\n",
    "    current_section = None\n",
    "    \n",
    "    # Parcourir les sections (rubriques) et les paragraphes\n",
    "    for tag in soup.find_all(['h2', 'p']):\n",
    "        if tag.name == 'h2':\n",
    "            current_section = tag.text.strip().replace(\"[edit]\", \"\")\n",
    "            content[current_section] = []\n",
    "        elif tag.name == 'p' and current_section:\n",
    "            content[current_section].append(tag.text.strip())\n",
    "\n",
    "    return content\n",
    "\n",
    "# 1.4) Fonction pour collecter chaque lien redirigeant vers une autre page Wikipédia\n",
    "def get_wikipedia_links(soup):\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href')\n",
    "        if href.startswith('/wiki/') and ':' not in href:  # Filtrer les liens internes valides\n",
    "            full_url = 'https://en.wikipedia.org' + href\n",
    "            links.append(full_url)\n",
    "    return links\n",
    "\n",
    "# 1.5) Fonction principale encapsulant toutes les autres\n",
    "def scrape_wikipedia(url):\n",
    "    soup = get_wikipedia_html(url)\n",
    "    \n",
    "    if soup:\n",
    "        # Extraction du titre\n",
    "        title = get_article_title(soup)\n",
    "        \n",
    "        # Extraction du contenu (rubriques et paragraphes)\n",
    "        content = get_article_content(soup)\n",
    "        \n",
    "        # Extraction des liens\n",
    "        links = get_wikipedia_links(soup)\n",
    "        \n",
    "        # Affichage des résultats\n",
    "        print(f\"Titre de l'article : {title}\\n\")\n",
    "        \n",
    "        print(\"Contenu de l'article :\")\n",
    "        for section, paragraphs in content.items():\n",
    "            print(f\"\\n--- {section} ---\")\n",
    "            for paragraph in paragraphs:\n",
    "                print(paragraph)\n",
    "        \n",
    "        print(\"\\nLiens Wikipedia collectés :\")\n",
    "        for link in links[:10]:  # Limiter à 10 liens pour l'affichage\n",
    "            print(link)\n",
    "\n",
    "# 1.6) Test de la fonction sur une page Wikipédia de votre choix\n",
    "url = 'https://en.wikipedia.org/wiki/Web_scraping'  # Exemple de page Wikipédia\n",
    "scrape_wikipedia(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b67b54-dc26-4a55-93d2-8621b9511af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a74ea2-abc6-427d-ad3d-4bfbb9ff44ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
